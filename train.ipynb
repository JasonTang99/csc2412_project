{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import os\n",
    "\n",
    "from utils import generate_run_id\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import opacus\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "\n",
    "# Random Seeding\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_1=64, hidden_2=16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size**2, hidden_1, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2, bias=False)\n",
    "        self.fc3 = nn.Linear(hidden_2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, image_size**2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Generator similar to DCGAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=100, ngf=32, nc=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.Conv2d(ngf, nc, kernel_size=5, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Setup Generator Weight Initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given parameter clip bounds c_p, compute maximal ReLU activation bounds B_sigma\n",
    "def compute_ReLU_bounds(model, c_p, input_size=(784,), input_bounds=1.0, B_sigma_p=1.0):\n",
    "    sample = torch.ones(image_size ** 2).to(device) * input_bounds\n",
    "    B_sigma = 0.0\n",
    "    sum_mk_mkp1 = 0\n",
    "    skip_first = True\n",
    "\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            W = torch.ones_like(layer.weight) * c_p\n",
    "            sample = W @ sample\n",
    "            \n",
    "            B_sigma = max(B_sigma, sample.max().detach().item())\n",
    "            \n",
    "            if skip_first:\n",
    "                skip_first = False\n",
    "            else:\n",
    "                sum_mk_mkp1 += W.shape[0] * W.shape[1]\n",
    "                # sum_mk_mkp1 += (W.shape[0] + 1) * (W.shape[1] + 1)\n",
    "            # print(layer.weight.shape, B_sigma, sample.max().detach().item(), sum_mk_mkp1, W.shape[0], W.shape[1])\n",
    "    print(\"B_sigma\", B_sigma)\n",
    "    print(\"sum_mk_mkp1\", sum_mk_mkp1)\n",
    "    \n",
    "    c_g = 2 * c_p * B_sigma * (B_sigma_p ** 2) * sum_mk_mkp1\n",
    "    return c_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/16-12_100_32_1_50.0_1e-06_0.25_0.01_0.001_0.5_32_3_10000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all parameters\n",
    "Args = namedtuple('Args', [\n",
    "    'hidden', 'nz', 'ngf', 'nc', 'epsilon', 'delta', 'noise_multiplier', \n",
    "    'c_p', 'lr', 'beta1', 'batch_size', 'n_d', 'n_g'\n",
    "])\n",
    "\n",
    "# n_d: number of discriminator updates per generator update\n",
    "# n_g: number of generator updates\n",
    "# args = Args(\n",
    "#     # Model Parameters\n",
    "#     hidden=[64, 16], nz=100, ngf=32, nc=1, \n",
    "#     # Privacy Parameters\n",
    "#     epsilon=50.0, delta=1e-6, noise_multiplier=0.1, c_p=0.01, \n",
    "#     # Training Parameters\n",
    "#     lr=1e-3, beta1=0.5, batch_size=32, n_d=3, n_g=int(1e4)\n",
    "# )\n",
    "\n",
    "args = Args(\n",
    "    # Model Parameters\n",
    "    hidden=[16, 12], nz=100, ngf=32, nc=1, \n",
    "    # Privacy Parameters\n",
    "    epsilon=50.0, delta=1e-6, noise_multiplier=0.3, c_p=0.01, \n",
    "    # Training Parameters\n",
    "    lr=1e-3, beta1=0.5, batch_size=16, n_d=3, n_g=int(1e4)\n",
    ")\n",
    "\n",
    "# Generate Run ID\n",
    "run_id = generate_run_id(args)\n",
    "\n",
    "# Create Folder Path\n",
    "run_fp = os.path.join('runs/', run_id)\n",
    "os.makedirs(run_fp, exist_ok=True)\n",
    "run_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_sigma 7.840000152587891\n",
      "sum_mk_mkp1 204\n",
      "Gradient clip: 31.987200622558593\n"
     ]
    }
   ],
   "source": [
    "# Setup models\n",
    "netD = Discriminator(args.hidden[0], args.hidden[1]).to(device)\n",
    "netG = Generator(args.nz, args.ngf, args.nc).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Privacy Validation\n",
    "ModuleValidator.validate(netD, strict=True)\n",
    "\n",
    "# Setup parameters for Gradient Clip Calculation\n",
    "c_g = compute_ReLU_bounds(netD, args.c_p)\n",
    "print(\"Gradient clip:\", c_g)\n",
    "\n",
    "# Setup optimizers\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "\n",
    "# Setup loss\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MNIST dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "M = len(train_set)\n",
    "\n",
    "# sample_prob = batch_size / M\n",
    "# print(\"Sample Probability:\", sample_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before make_private(). Model:<class '__main__.Discriminator'>, \n",
      "Optimizer:<class 'torch.optim.adam.Adam'>, \n",
      "DataLoader:<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "====================\n",
      "After make_private(). Model:<class 'opacus.grad_sample.grad_sample_module.GradSampleModule'>, \n",
      "Optimizer:<class 'opacus.optimizers.optimizer.DPOptimizer'>, \n",
      "DataLoader:<class 'opacus.data_loader.DPDataLoader'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/.pyenv/versions/3.8.0/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "privacy_engine = PrivacyEngine()\n",
    "print(\n",
    "    f\"Before make_private(). \"\n",
    "    f\"Model:{type(netD)}, \\nOptimizer:{type(optimizerD)}, \\nDataLoader:{type(train_loader)}\"\n",
    ")\n",
    "netD, optimizerD, train_loader = privacy_engine.make_private(\n",
    "    module=netD,\n",
    "    optimizer=optimizerD,\n",
    "    data_loader=train_loader,\n",
    "    max_grad_norm=c_g,\n",
    "    noise_multiplier=args.noise_multiplier,\n",
    ")\n",
    "print(\"=\"*20)\n",
    "print(\n",
    "    f\"After make_private(). \"\n",
    "    f\"Model:{type(netD)}, \\nOptimizer:{type(optimizerD)}, \\nDataLoader:{type(train_loader)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(run_fp, args, netD, netG, optimizerD, optimizerG, loss_fn, train_loader, device, verbose=False):\n",
    "    netD.train()\n",
    "    netG.train()\n",
    "\n",
    "    print_mod = 10\n",
    "\n",
    "    with open(f\"{run_fp}/loss.txt\", \"a\") as f:\n",
    "        for i in tqdm(range(args.n_g)):\n",
    "            # Update Discriminator\n",
    "            netD.enable_hooks()\n",
    "            netD.train()\n",
    "            netG.eval()\n",
    "            for j in range(args.n_d):\n",
    "                # Train with real data\n",
    "                real_data = next(iter(train_loader))[0].to(device)\n",
    "                real_data = real_data.view(real_data.size(0), -1)\n",
    "                \n",
    "                # Train with fake data\n",
    "                noise = torch.randn(real_data.size(0), 100, 1, 1).to(device)\n",
    "                fake_data = netG(noise)\n",
    "\n",
    "                # Run Discriminator\n",
    "                real_output = netD(real_data)\n",
    "                fake_output = netD(fake_data)\n",
    "\n",
    "                # Calculate loss (Wasserstein Loss)\n",
    "                d_loss = -torch.mean(real_output) + torch.mean(fake_output)\n",
    "\n",
    "                # real_label = torch.ones(real_data.size(0), 1).to(device)\n",
    "                # fake_label = torch.zeros(real_data.size(0), 1).to(device)\n",
    "\n",
    "                # Update Discriminator\n",
    "                # real_loss = loss_fn(real_output, real_label)\n",
    "                # fake_loss = loss_fn(fake_output, fake_label)\n",
    "                # d_loss = real_loss + fake_loss\n",
    "\n",
    "                optimizerD.zero_grad()\n",
    "                d_loss.backward()\n",
    "                optimizerD.step()\n",
    "\n",
    "                # Clip weights in discriminator\n",
    "                for p in netD.parameters():\n",
    "                    p.data.clamp_(-args.c_p, args.c_p)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"Epoch: {i} ({j}/{args.n_d}) D_loss: {d_loss.item()} eps: {privacy_engine.get_epsilon(args.delta)}\")\n",
    "                if i % print_mod == 0:\n",
    "                    print(f\"{i}.{j}, {d_loss.item()}, {privacy_engine.get_epsilon(args.delta)}\", file=f)\n",
    "\n",
    "            # Make a copy of the discriminator\n",
    "            # copy_dict = netD.state_dict()\n",
    "            # copy_dict = {k.replace('_module.', ''): v for k, v in copy_dict.items()}\n",
    "            # netD_copy.load_state_dict(copy_dict)\n",
    "\n",
    "            netD.eval()\n",
    "            # for p in netD.parameters():\n",
    "            #     p.requires_grad = False\n",
    "            netD.disable_hooks()\n",
    "            netG.train()\n",
    "\n",
    "            # Update Generator\n",
    "            noise = torch.randn(args.batch_size, 100, 1, 1).to(device)\n",
    "            fake_data = netG(noise)\n",
    "            # fake_output = netD_copy(fake_data)\n",
    "            fake_output = netD(fake_data)\n",
    "            fake_target = torch.ones(args.batch_size, 1).to(device)\n",
    "            g_loss = loss_fn(fake_output, fake_target)\n",
    "\n",
    "            # Update Generator\n",
    "            optimizerG.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} G_loss: {g_loss.item()}\")\n",
    "            if i % print_mod == 0:\n",
    "                print(f\"{i}, {g_loss.item()}\", file=f)\n",
    "\n",
    "            if (i+1) % 2000 == 0:\n",
    "                torch.save(netG.state_dict(), f\"{run_fp}/netG_{i+1}.pt\")\n",
    "                torch.save(netD._module.state_dict(), f\"{run_fp}/netD_{i+1}.pt\")\n",
    "\n",
    "                # Save the model\n",
    "                torch.save(privacy_engine.accountant, f\"{run_fp}/accountant_{i+1}.pth\")\n",
    "                torch.save(optimizerD.original_optimizer.state_dict(), f\"{run_fp}/optimizerD_{i+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_fp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create empty loss file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrun_fp\u001b[39m}\u001b[39;00m\u001b[39m/loss.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train(run_fp, args, netD, netG, optimizerD, optimizerG, loss_fn, train_loader, device, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_fp' is not defined"
     ]
    }
   ],
   "source": [
    "# Create empty loss file\n",
    "with open(f\"{run_fp}/loss.txt\", \"w\") as f:\n",
    "    pass\n",
    "\n",
    "train(run_fp, args, netD, netG, optimizerD, optimizerG, loss_fn, train_loader, device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Max Norm: tensor(1.1140, device='cuda:0') 0 0\n",
      "Max Norm: tensor(1.1140, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Verification of gradient clipping\n",
    "def param_grad_norm(model):\n",
    "    gradient_norm = 0\n",
    "    for param in model.parameters():\n",
    "        gradient_norm += torch.sum(param.grad ** 2)\n",
    "    gradient_norm = gradient_norm ** 0.5\n",
    "    return gradient_norm\n",
    "\n",
    "def param_grad_l1(model):\n",
    "    gradient_norm = 0\n",
    "    for param in model.parameters():\n",
    "        gradient_norm += param.grad.abs().sum().item()\n",
    "    return gradient_norm\n",
    "\n",
    "from models import Discriminator\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Discriminator([16, 12], input_size=784).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "c_g = 1.1139578420722147\n",
    "\n",
    "max_norm = 0\n",
    "first = True\n",
    "epsilon = 1e-7\n",
    "c_p = 0.01\n",
    "for idx in range(1000):\n",
    "    if first:\n",
    "        fill_val = c_p\n",
    "        # Set model weights to c_p\n",
    "        for p in model.parameters():\n",
    "            p.data.fill_(fill_val)\n",
    "        first = False\n",
    "    else:\n",
    "        # Randomize model weights (clip to c_p)\n",
    "        for p in model.parameters():\n",
    "            # randomize then clamp\n",
    "            p.data = torch.clamp(torch.randn_like(p.data)* 5, -c_p, c_p)\n",
    "    \n",
    "    for c in range(2):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # random sample\n",
    "        sample = (torch.rand(784) > 0.0).to(torch.float32).to(device)\n",
    "        # sample = torch.ones(784).to(device)\n",
    "        assert sample.min().item() >= 0.0\n",
    "        assert sample.max().item() <= 1.0\n",
    "\n",
    "        sample_out = model(sample)\n",
    "        # print(\"Sample Out:\", sample_out.shape)\n",
    "\n",
    "        # Assert all activations are below B_sigma\n",
    "        \n",
    "\n",
    "        # activated_1 = F.relu(model.fc1(sample))\n",
    "        # activated_2 = F.relu(model.fc2(activated_1))\n",
    "        # activated_3 = torch.sigmoid(model.fc3(activated_2))\n",
    "        # assert activated_1.max().item() <= B_sigma\n",
    "        # assert activated_2.max().item() <= B_sigma\n",
    "        # assert activated_3.max().item() <= B_sigma\n",
    "        \n",
    "        target = torch.ones((1, 1)).to(device) * c\n",
    "\n",
    "        # loss = loss_fn(sample_out, target)\n",
    "        loss = torch.sum(sample_out)\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = param_grad_norm(model)\n",
    "        # l1_norm = param_grad_l1(model)\n",
    "        if grad_norm > max_norm:\n",
    "            max_norm = grad_norm\n",
    "            # print(\"New L1 Norm:\", l1_norm, idx, c)\n",
    "            print(\"New Max Norm:\", max_norm, idx, c)\n",
    "    # break\n",
    "    if max_norm > c_g:\n",
    "        print(\"Max Norm Exceeded\")\n",
    "        break\n",
    "\n",
    "print(\"Max Norm:\", max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15e8192faefa7c4df086f69d5df89c8d0b9f28f8905c5b10f9ee705b3f14aede"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
